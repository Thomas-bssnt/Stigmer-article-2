# Promoting cooperation or defection in human groups with simple social bots

Thomas Bassanetti, Stéphane Cezera, Maxime Delacroix, Ramón Escobedo, Adrien Blanchet, Clément Sire, and Guy Theraulaz

In the digital era, human cooperation is increasingly mediated by indirect social cues such as ratings, reviews, and other digital traces left in online environments. These traces often guide collective behavior via stigmergy, a coordination mechanism whereby individuals interact through modifications of a shared environment. In this study, we explore how simple model-driven bots can influence human cooperation or defection in a competitive rating game inspired by online marketplaces. Participants, unaware of the bots' presence, interacted with either four human partners or four bots exhibiting predefined behaviors—cooperative, neutral, deceptive, or optimized for group performance. We show that the presence and behavior of bots significantly affect human strategies and performance. Higher levels of cooperation among bots improve human outcomes but also increase the frequency of deceptive human strategies, suggesting exploitation of reliable social information. Conversely, in less cooperative environments, participants adopt more collaborative or neutral behaviors to preserve informational value. By classifying individuals into three behavioral profiles—collaborators, neutrals, and defectors—we develop a linear regression model using three cues: the average value of rated cells, the diversity of rated cells, and the player's rank. These cues allow accurate prediction of behavioral profile distributions across experimental conditions. An adaptive agent-based model further reproduces the empirical results. Our findings demonstrate that even simple bots can strongly influence collective dynamics in human groups. These insights have implications for the design of recommendation systems, the regulation of automated agents, and the understanding of cooperation and deception in digital societies.
